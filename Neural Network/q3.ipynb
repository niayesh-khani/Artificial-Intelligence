{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xK1IW0mr4kMZ",
        "outputId": "d6c668f3-dd84-49e3-a33a-21566175da7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Iteration 1, loss = 26.44171147\n",
            "Iteration 2, loss = 5.03168590\n",
            "Iteration 3, loss = 3.27600237\n",
            "Iteration 4, loss = 3.17852484\n",
            "Iteration 5, loss = 3.07270290\n",
            "Iteration 6, loss = 3.01399688\n",
            "Iteration 7, loss = 2.98174158\n",
            "Iteration 8, loss = 2.93175138\n",
            "Iteration 9, loss = 2.86976009\n",
            "Iteration 10, loss = 2.81384760\n",
            "Iteration 11, loss = 2.76449744\n",
            "Iteration 12, loss = 2.73585528\n",
            "Iteration 13, loss = 2.71352951\n",
            "Iteration 14, loss = 2.67152214\n",
            "Iteration 15, loss = 2.65472189\n",
            "Iteration 16, loss = 2.62286468\n",
            "Iteration 17, loss = 2.57970698\n",
            "Iteration 18, loss = 2.54458896\n",
            "Iteration 19, loss = 2.52492688\n",
            "Iteration 20, loss = 2.48989076\n",
            "Iteration 21, loss = 2.46716111\n",
            "Iteration 22, loss = 2.43692853\n",
            "Iteration 23, loss = 2.44621567\n",
            "Iteration 24, loss = 2.40146378\n",
            "Iteration 25, loss = 2.37112119\n",
            "Iteration 26, loss = 2.36302932\n",
            "Iteration 27, loss = 2.33748244\n",
            "Iteration 28, loss = 2.31867760\n",
            "Iteration 29, loss = 2.28562286\n",
            "Iteration 30, loss = 2.26538898\n",
            "Iteration 31, loss = 2.24378810\n",
            "Iteration 32, loss = 2.21529308\n",
            "Iteration 33, loss = 2.20831623\n",
            "Iteration 34, loss = 2.22277084\n",
            "Iteration 35, loss = 2.18834312\n",
            "Iteration 36, loss = 2.14400573\n",
            "Iteration 37, loss = 2.12874447\n",
            "Iteration 38, loss = 2.10624411\n",
            "Iteration 39, loss = 2.10506265\n",
            "Iteration 40, loss = 2.07348092\n",
            "Iteration 41, loss = 2.06028500\n",
            "Iteration 42, loss = 2.04954395\n",
            "Iteration 43, loss = 2.03352354\n",
            "Iteration 44, loss = 2.02391298\n",
            "Iteration 45, loss = 2.01574376\n",
            "Iteration 46, loss = 1.99944771\n",
            "Iteration 47, loss = 1.99812107\n",
            "Iteration 48, loss = 1.98124977\n",
            "Iteration 49, loss = 1.96406604\n",
            "Iteration 50, loss = 1.94850334\n",
            "Iteration 51, loss = 1.91823409\n",
            "Iteration 52, loss = 1.90012133\n",
            "Iteration 53, loss = 1.89527477\n",
            "Iteration 54, loss = 1.86996592\n",
            "Iteration 55, loss = 1.84894059\n",
            "Iteration 56, loss = 1.83021859\n",
            "Iteration 57, loss = 1.81903028\n",
            "Iteration 58, loss = 1.80447109\n",
            "Iteration 59, loss = 1.79108751\n",
            "Iteration 60, loss = 1.77695385\n",
            "Iteration 61, loss = 1.76041388\n",
            "Iteration 62, loss = 1.74449246\n",
            "Iteration 63, loss = 1.72379212\n",
            "Iteration 64, loss = 1.70361075\n",
            "Iteration 65, loss = 1.68210745\n",
            "Iteration 66, loss = 1.67844609\n",
            "Iteration 67, loss = 1.65514328\n",
            "Iteration 68, loss = 1.63939122\n",
            "Iteration 69, loss = 1.61409077\n",
            "Iteration 70, loss = 1.60036020\n",
            "Iteration 71, loss = 1.59015747\n",
            "Iteration 72, loss = 1.56386165\n",
            "Iteration 73, loss = 1.55465558\n",
            "Iteration 74, loss = 1.53916390\n",
            "Iteration 75, loss = 1.51812694\n",
            "Iteration 76, loss = 1.50519970\n",
            "Iteration 77, loss = 1.50696457\n",
            "Iteration 78, loss = 1.48017134\n",
            "Iteration 79, loss = 1.45522643\n",
            "Iteration 80, loss = 1.44737683\n",
            "Iteration 81, loss = 1.42434966\n",
            "Iteration 82, loss = 1.41358824\n",
            "Iteration 83, loss = 1.40332035\n",
            "Iteration 84, loss = 1.39162854\n",
            "Iteration 85, loss = 1.37326575\n",
            "Iteration 86, loss = 1.37254675\n",
            "Iteration 87, loss = 1.35089913\n",
            "Iteration 88, loss = 1.33184529\n",
            "Iteration 89, loss = 1.32715706\n",
            "Iteration 90, loss = 1.31058878\n",
            "Iteration 91, loss = 1.30106471\n",
            "Iteration 92, loss = 1.28692434\n",
            "Iteration 93, loss = 1.28000315\n",
            "Iteration 94, loss = 1.28137909\n",
            "Iteration 95, loss = 1.27131814\n",
            "Iteration 96, loss = 1.26529237\n",
            "Iteration 97, loss = 1.25503150\n",
            "Iteration 98, loss = 1.24072986\n",
            "Iteration 99, loss = 1.23725712\n",
            "Iteration 100, loss = 1.22955034\n",
            "Iteration 101, loss = 1.20998353\n",
            "Iteration 102, loss = 1.20143375\n",
            "Iteration 103, loss = 1.19490180\n",
            "Iteration 104, loss = 1.19122418\n",
            "Iteration 105, loss = 1.18028965\n",
            "Iteration 106, loss = 1.16753648\n",
            "Iteration 107, loss = 1.15601551\n",
            "Iteration 108, loss = 1.15162315\n",
            "Iteration 109, loss = 1.14693439\n",
            "Iteration 110, loss = 1.14099039\n",
            "Iteration 111, loss = 1.13906548\n",
            "Iteration 112, loss = 1.13114713\n",
            "Iteration 113, loss = 1.12644222\n",
            "Iteration 114, loss = 1.12851392\n",
            "Iteration 115, loss = 1.11035774\n",
            "Iteration 116, loss = 1.10136995\n",
            "Iteration 117, loss = 1.09311679\n",
            "Iteration 118, loss = 1.09256832\n",
            "Iteration 119, loss = 1.09421053\n",
            "Iteration 120, loss = 1.08094597\n",
            "Iteration 121, loss = 1.09000962\n",
            "Iteration 122, loss = 1.07789945\n",
            "Iteration 123, loss = 1.06556819\n",
            "Iteration 124, loss = 1.05869572\n",
            "Iteration 125, loss = 1.05646219\n",
            "Iteration 126, loss = 1.05239729\n",
            "Iteration 127, loss = 1.04384740\n",
            "Iteration 128, loss = 1.04163047\n",
            "Iteration 129, loss = 1.02982997\n",
            "Iteration 130, loss = 1.03049178\n",
            "Iteration 131, loss = 1.01438943\n",
            "Iteration 132, loss = 1.00744292\n",
            "Iteration 133, loss = 0.99534375\n",
            "Iteration 134, loss = 0.99112646\n",
            "Iteration 135, loss = 0.98081591\n",
            "Iteration 136, loss = 0.97379559\n",
            "Iteration 137, loss = 0.97776036\n",
            "Iteration 138, loss = 0.97311050\n",
            "Iteration 139, loss = 0.96506574\n",
            "Iteration 140, loss = 0.96246089\n",
            "Iteration 141, loss = 0.96232386\n",
            "Iteration 142, loss = 0.95953537\n",
            "Iteration 143, loss = 0.95794396\n",
            "Iteration 144, loss = 0.97376213\n",
            "Iteration 145, loss = 0.94953702\n",
            "Iteration 146, loss = 0.93914803\n",
            "Iteration 147, loss = 0.93224778\n",
            "Iteration 148, loss = 0.92982016\n",
            "Iteration 149, loss = 0.94250117\n",
            "Iteration 150, loss = 0.93228411\n",
            "Iteration 151, loss = 0.92189888\n",
            "Iteration 152, loss = 0.92019962\n",
            "Iteration 153, loss = 0.92329486\n",
            "Iteration 154, loss = 0.91102747\n",
            "Iteration 155, loss = 0.89804761\n",
            "Iteration 156, loss = 0.89245724\n",
            "Iteration 157, loss = 0.88279969\n",
            "Iteration 158, loss = 0.88787154\n",
            "Iteration 159, loss = 0.87606276\n",
            "Iteration 160, loss = 0.86308728\n",
            "Iteration 161, loss = 0.86309399\n",
            "Iteration 162, loss = 0.85311096\n",
            "Iteration 163, loss = 0.84715089\n",
            "Iteration 164, loss = 0.83734786\n",
            "Iteration 165, loss = 0.83079201\n",
            "Iteration 166, loss = 0.82339410\n",
            "Iteration 167, loss = 0.81567395\n",
            "Iteration 168, loss = 0.81518548\n",
            "Iteration 169, loss = 0.80762898\n",
            "Iteration 170, loss = 0.80878769\n",
            "Iteration 171, loss = 0.80228242\n",
            "Iteration 172, loss = 0.81414785\n",
            "Iteration 173, loss = 0.80007056\n",
            "Iteration 174, loss = 0.78852684\n",
            "Iteration 175, loss = 0.77999259\n",
            "Iteration 176, loss = 0.77672929\n",
            "Iteration 177, loss = 0.76439843\n",
            "Iteration 178, loss = 0.75956588\n",
            "Iteration 179, loss = 0.74977086\n",
            "Iteration 180, loss = 0.74277981\n",
            "Iteration 181, loss = 0.73814507\n",
            "Iteration 182, loss = 0.73617631\n",
            "Iteration 183, loss = 0.73108716\n",
            "Iteration 184, loss = 0.72326796\n",
            "Iteration 185, loss = 0.71524135\n",
            "Iteration 186, loss = 0.72298591\n",
            "Iteration 187, loss = 0.71304504\n",
            "Iteration 188, loss = 0.70687438\n",
            "Iteration 189, loss = 0.70183650\n",
            "Iteration 190, loss = 0.69671807\n",
            "Iteration 191, loss = 0.69415550\n",
            "Iteration 192, loss = 0.69485435\n",
            "Iteration 193, loss = 0.69148373\n",
            "Iteration 194, loss = 0.70220596\n",
            "Iteration 195, loss = 0.69483019\n",
            "Iteration 196, loss = 0.70342420\n",
            "Iteration 197, loss = 0.70359551\n",
            "Iteration 198, loss = 0.68830603\n",
            "Iteration 199, loss = 0.68189914\n",
            "Iteration 200, loss = 0.67246809\n",
            "Iteration 201, loss = 0.67701890\n",
            "Iteration 202, loss = 0.67041037\n",
            "Iteration 203, loss = 0.67897921\n",
            "Iteration 204, loss = 0.70479506\n",
            "Iteration 205, loss = 0.68420549\n",
            "Iteration 206, loss = 0.68114323\n",
            "Iteration 207, loss = 0.67385306\n",
            "Iteration 208, loss = 0.67182722\n",
            "Iteration 209, loss = 0.69284680\n",
            "Iteration 210, loss = 0.68007433\n",
            "Iteration 211, loss = 0.66304070\n",
            "Iteration 212, loss = 0.65283827\n",
            "Iteration 213, loss = 0.64636827\n",
            "Iteration 214, loss = 0.63792817\n",
            "Iteration 215, loss = 0.63140818\n",
            "Iteration 216, loss = 0.62010883\n",
            "Iteration 217, loss = 0.61811193\n",
            "Iteration 218, loss = 0.61422484\n",
            "Iteration 219, loss = 0.61345796\n",
            "Iteration 220, loss = 0.61202490\n",
            "Iteration 221, loss = 0.60809028\n",
            "Iteration 222, loss = 0.60720381\n",
            "Iteration 223, loss = 0.60618730\n",
            "Iteration 224, loss = 0.60563177\n",
            "Iteration 225, loss = 0.60226385\n",
            "Iteration 226, loss = 0.59980097\n",
            "Iteration 227, loss = 0.59908127\n",
            "Iteration 228, loss = 0.59247176\n",
            "Iteration 229, loss = 0.59216315\n",
            "Iteration 230, loss = 0.58839965\n",
            "Iteration 231, loss = 0.59004586\n",
            "Iteration 232, loss = 0.58832590\n",
            "Iteration 233, loss = 0.58752724\n",
            "Iteration 234, loss = 0.58384976\n",
            "Iteration 235, loss = 0.58306812\n",
            "Iteration 236, loss = 0.58215607\n",
            "Iteration 237, loss = 0.58280775\n",
            "Iteration 238, loss = 0.58033180\n",
            "Iteration 239, loss = 0.58057931\n",
            "Iteration 240, loss = 0.57619342\n",
            "Iteration 241, loss = 0.57467809\n",
            "Iteration 242, loss = 0.57742057\n",
            "Iteration 243, loss = 0.57321171\n",
            "Iteration 244, loss = 0.57684127\n",
            "Iteration 245, loss = 0.57379802\n",
            "Iteration 246, loss = 0.56386492\n",
            "Iteration 247, loss = 0.56536939\n",
            "Iteration 248, loss = 0.56393308\n",
            "Iteration 249, loss = 0.56532227\n",
            "Iteration 250, loss = 0.56404496\n",
            "Iteration 251, loss = 0.56474518\n",
            "Iteration 252, loss = 0.55817618\n",
            "Iteration 253, loss = 0.55374791\n",
            "Iteration 254, loss = 0.55406923\n",
            "Iteration 255, loss = 0.55459713\n",
            "Iteration 256, loss = 0.57519134\n",
            "Iteration 257, loss = 0.56240130\n",
            "Iteration 258, loss = 0.55694251\n",
            "Iteration 259, loss = 0.55254563\n",
            "Iteration 260, loss = 0.54114113\n",
            "Iteration 261, loss = 0.53315317\n",
            "Iteration 262, loss = 0.54295104\n",
            "Iteration 263, loss = 0.54004717\n",
            "Iteration 264, loss = 0.53054097\n",
            "Iteration 265, loss = 0.54875945\n",
            "Iteration 266, loss = 0.53648114\n",
            "Iteration 267, loss = 0.52528246\n",
            "Iteration 268, loss = 0.51573806\n",
            "Iteration 269, loss = 0.52278228\n",
            "Iteration 270, loss = 0.51703279\n",
            "Iteration 271, loss = 0.51116745\n",
            "Iteration 272, loss = 0.50765974\n",
            "Iteration 273, loss = 0.50863156\n",
            "Iteration 274, loss = 0.50635142\n",
            "Iteration 275, loss = 0.50722904\n",
            "Iteration 276, loss = 0.50757336\n",
            "Iteration 277, loss = 0.50865719\n",
            "Iteration 278, loss = 0.50292956\n",
            "Iteration 279, loss = 0.49665938\n",
            "Iteration 280, loss = 0.49937032\n",
            "Iteration 281, loss = 0.49455635\n",
            "Iteration 282, loss = 0.49253277\n",
            "Iteration 283, loss = 0.49658356\n",
            "Iteration 284, loss = 0.49881249\n",
            "Iteration 285, loss = 0.49179118\n",
            "Iteration 286, loss = 0.48929196\n",
            "Iteration 287, loss = 0.48604834\n",
            "Iteration 288, loss = 0.48611833\n",
            "Iteration 289, loss = 0.48415061\n",
            "Iteration 290, loss = 0.47980394\n",
            "Iteration 291, loss = 0.47873219\n",
            "Iteration 292, loss = 0.47634895\n",
            "Iteration 293, loss = 0.47531069\n",
            "Iteration 294, loss = 0.47943717\n",
            "Iteration 295, loss = 0.47740680\n",
            "Iteration 296, loss = 0.47797647\n",
            "Iteration 297, loss = 0.48120172\n",
            "Iteration 298, loss = 0.47613665\n",
            "Iteration 299, loss = 0.47277119\n",
            "Iteration 300, loss = 0.46918792\n",
            "Iteration 301, loss = 0.47179246\n",
            "Iteration 302, loss = 0.48268676\n",
            "Iteration 303, loss = 0.47596799\n",
            "Iteration 304, loss = 0.47355018\n",
            "Iteration 305, loss = 0.46913009\n",
            "Iteration 306, loss = 0.46932472\n",
            "Iteration 307, loss = 0.46514809\n",
            "Iteration 308, loss = 0.46431975\n",
            "Iteration 309, loss = 0.46734182\n",
            "Iteration 310, loss = 0.45541017\n",
            "Iteration 311, loss = 0.45171039\n",
            "Iteration 312, loss = 0.45991645\n",
            "Iteration 313, loss = 0.45705002\n",
            "Iteration 314, loss = 0.45070936\n",
            "Iteration 315, loss = 0.45084278\n",
            "Iteration 316, loss = 0.45770951\n",
            "Iteration 317, loss = 0.45166438\n",
            "Iteration 318, loss = 0.44794056\n",
            "Iteration 319, loss = 0.43540935\n",
            "Iteration 320, loss = 0.43435422\n",
            "Iteration 321, loss = 0.43328878\n",
            "Iteration 322, loss = 0.42618936\n",
            "Iteration 323, loss = 0.42296948\n",
            "Iteration 324, loss = 0.42496456\n",
            "Iteration 325, loss = 0.42389661\n",
            "Iteration 326, loss = 0.41773333\n",
            "Iteration 327, loss = 0.41499030\n",
            "Iteration 328, loss = 0.41858343\n",
            "Iteration 329, loss = 0.41466163\n",
            "Iteration 330, loss = 0.41236486\n",
            "Iteration 331, loss = 0.40973453\n",
            "Iteration 332, loss = 0.41048063\n",
            "Iteration 333, loss = 0.41595511\n",
            "Iteration 334, loss = 0.41267372\n",
            "Iteration 335, loss = 0.40837529\n",
            "Iteration 336, loss = 0.40394855\n",
            "Iteration 337, loss = 0.41027241\n",
            "Iteration 338, loss = 0.41421340\n",
            "Iteration 339, loss = 0.40398422\n",
            "Iteration 340, loss = 0.40096787\n",
            "Iteration 341, loss = 0.39856240\n",
            "Iteration 342, loss = 0.41517369\n",
            "Iteration 343, loss = 0.40217976\n",
            "Iteration 344, loss = 0.40112903\n",
            "Iteration 345, loss = 0.39578001\n",
            "Iteration 346, loss = 0.39284863\n",
            "Iteration 347, loss = 0.39074478\n",
            "Iteration 348, loss = 0.39152354\n",
            "Iteration 349, loss = 0.38732284\n",
            "Iteration 350, loss = 0.38748190\n",
            "Iteration 351, loss = 0.38496860\n",
            "Iteration 352, loss = 0.38197976\n",
            "Iteration 353, loss = 0.38071081\n",
            "Iteration 354, loss = 0.38211325\n",
            "Iteration 355, loss = 0.37802215\n",
            "Iteration 356, loss = 0.37902297\n",
            "Iteration 357, loss = 0.37523208\n",
            "Iteration 358, loss = 0.37761503\n",
            "Iteration 359, loss = 0.38062377\n",
            "Iteration 360, loss = 0.37804647\n",
            "Iteration 361, loss = 0.37280998\n",
            "Iteration 362, loss = 0.37633548\n",
            "Iteration 363, loss = 0.37546986\n",
            "Iteration 364, loss = 0.37860272\n",
            "Iteration 365, loss = 0.37788601\n",
            "Iteration 366, loss = 0.37555136\n",
            "Iteration 367, loss = 0.37266822\n",
            "Iteration 368, loss = 0.36950241\n",
            "Iteration 369, loss = 0.37204023\n",
            "Iteration 370, loss = 0.37221441\n",
            "Iteration 371, loss = 0.37064802\n",
            "Iteration 372, loss = 0.36752279\n",
            "Iteration 373, loss = 0.36594388\n",
            "Iteration 374, loss = 0.36736493\n",
            "Iteration 375, loss = 0.36644191\n",
            "Iteration 376, loss = 0.36659460\n",
            "Iteration 377, loss = 0.36512078\n",
            "Iteration 378, loss = 0.36165007\n",
            "Iteration 379, loss = 0.37636704\n",
            "Iteration 380, loss = 0.36647989\n",
            "Iteration 381, loss = 0.36235869\n",
            "Iteration 382, loss = 0.36433723\n",
            "Iteration 383, loss = 0.36157922\n",
            "Iteration 384, loss = 0.35903487\n",
            "Iteration 385, loss = 0.35684948\n",
            "Iteration 386, loss = 0.35555889\n",
            "Iteration 387, loss = 0.35431256\n",
            "Iteration 388, loss = 0.35659655\n",
            "Iteration 389, loss = 0.35370352\n",
            "Iteration 390, loss = 0.35400527\n",
            "Iteration 391, loss = 0.35327730\n",
            "Iteration 392, loss = 0.35607500\n",
            "Iteration 393, loss = 0.35353304\n",
            "Iteration 394, loss = 0.35044901\n",
            "Iteration 395, loss = 0.35177184\n",
            "Iteration 396, loss = 0.37361115\n",
            "Iteration 397, loss = 0.36994115\n",
            "Iteration 398, loss = 0.36484534\n",
            "Iteration 399, loss = 0.36563321\n",
            "Iteration 400, loss = 0.35795046\n",
            "Iteration 401, loss = 0.35096472\n",
            "Iteration 402, loss = 0.34929707\n",
            "Iteration 403, loss = 0.34800847\n",
            "Iteration 404, loss = 0.34838410\n",
            "Iteration 405, loss = 0.35065171\n",
            "Iteration 406, loss = 0.35613154\n",
            "Iteration 407, loss = 0.35026077\n",
            "Iteration 408, loss = 0.37199803\n",
            "Iteration 409, loss = 0.35248437\n",
            "Iteration 410, loss = 0.35124416\n",
            "Iteration 411, loss = 0.34443773\n",
            "Iteration 412, loss = 0.34146580\n",
            "Iteration 413, loss = 0.34500424\n",
            "Iteration 414, loss = 0.33946964\n",
            "Iteration 415, loss = 0.33737088\n",
            "Iteration 416, loss = 0.34001977\n",
            "Iteration 417, loss = 0.33928301\n",
            "Iteration 418, loss = 0.33206989\n",
            "Iteration 419, loss = 0.33236286\n",
            "Iteration 420, loss = 0.32927644\n",
            "Iteration 421, loss = 0.32542913\n",
            "Iteration 422, loss = 0.32783826\n",
            "Iteration 423, loss = 0.32452929\n",
            "Iteration 424, loss = 0.32288057\n",
            "Iteration 425, loss = 0.32105480\n",
            "Iteration 426, loss = 0.32197489\n",
            "Iteration 427, loss = 0.32298670\n",
            "Iteration 428, loss = 0.32204487\n",
            "Iteration 429, loss = 0.32056894\n",
            "Iteration 430, loss = 0.32263883\n",
            "Iteration 431, loss = 0.32007913\n",
            "Iteration 432, loss = 0.31888527\n",
            "Iteration 433, loss = 0.32218356\n",
            "Iteration 434, loss = 0.31921034\n",
            "Iteration 435, loss = 0.31597910\n",
            "Iteration 436, loss = 0.31666595\n",
            "Iteration 437, loss = 0.31406619\n",
            "Iteration 438, loss = 0.31533093\n",
            "Iteration 439, loss = 0.32423299\n",
            "Iteration 440, loss = 0.31593405\n",
            "Iteration 441, loss = 0.31320734\n",
            "Iteration 442, loss = 0.31387280\n",
            "Iteration 443, loss = 0.31175091\n",
            "Iteration 444, loss = 0.31316541\n",
            "Iteration 445, loss = 0.32808501\n",
            "Iteration 446, loss = 0.32659585\n",
            "Iteration 447, loss = 0.32235312\n",
            "Iteration 448, loss = 0.32397819\n",
            "Iteration 449, loss = 0.31965888\n",
            "Iteration 450, loss = 0.31764198\n",
            "Iteration 451, loss = 0.31268845\n",
            "Iteration 452, loss = 0.31054331\n",
            "Iteration 453, loss = 0.31352948\n",
            "Iteration 454, loss = 0.33086019\n",
            "Iteration 455, loss = 0.33913794\n",
            "Iteration 456, loss = 0.33247923\n",
            "Iteration 457, loss = 0.31580068\n",
            "Iteration 458, loss = 0.31049897\n",
            "Iteration 459, loss = 0.30705936\n",
            "Iteration 460, loss = 0.30683004\n",
            "Iteration 461, loss = 0.30063615\n",
            "Iteration 462, loss = 0.29793162\n",
            "Iteration 463, loss = 0.29565753\n",
            "Iteration 464, loss = 0.30203511\n",
            "Iteration 465, loss = 0.30074941\n",
            "Iteration 466, loss = 0.29364774\n",
            "Iteration 467, loss = 0.29572342\n",
            "Iteration 468, loss = 0.29788042\n",
            "Iteration 469, loss = 0.29789778\n",
            "Iteration 470, loss = 0.29248643\n",
            "Iteration 471, loss = 0.29955622\n",
            "Iteration 472, loss = 0.29259327\n",
            "Iteration 473, loss = 0.28616265\n",
            "Iteration 474, loss = 0.27876285\n",
            "Iteration 475, loss = 0.27992678\n",
            "Iteration 476, loss = 0.27814751\n",
            "Iteration 477, loss = 0.27289728\n",
            "Iteration 478, loss = 0.27450021\n",
            "Iteration 479, loss = 0.27176790\n",
            "Iteration 480, loss = 0.26838941\n",
            "Iteration 481, loss = 0.26780118\n",
            "Iteration 482, loss = 0.26902521\n",
            "Iteration 483, loss = 0.27199134\n",
            "Iteration 484, loss = 0.26669481\n",
            "Iteration 485, loss = 0.26987562\n",
            "Iteration 486, loss = 0.27016195\n",
            "Iteration 487, loss = 0.26327273\n",
            "Iteration 488, loss = 0.26182394\n",
            "Iteration 489, loss = 0.26341020\n",
            "Iteration 490, loss = 0.26251348\n",
            "Iteration 491, loss = 0.25846294\n",
            "Iteration 492, loss = 0.25503224\n",
            "Iteration 493, loss = 0.25485362\n",
            "Iteration 494, loss = 0.25640443\n",
            "Iteration 495, loss = 0.25267759\n",
            "Iteration 496, loss = 0.25354487\n",
            "Iteration 497, loss = 0.25332021\n",
            "Iteration 498, loss = 0.25790967\n",
            "Iteration 499, loss = 0.25466994\n",
            "Iteration 500, loss = 0.25369753\n",
            "The accuracy of the model is 39.37621832358674%.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.decomposition import PCA\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "import csv\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# # Unzip the dataset\n",
        "# with zipfile.ZipFile('/content/drive/MyDrive/Dataset.zip', 'r') as zip_ref:\n",
        "#     zip_ref.extractall('/content/drive/MyDrive')\n",
        "\n",
        "# Define a function to load images and labels\n",
        "def load_data(image_folder, label_file):\n",
        "    images = []\n",
        "    labels = []\n",
        "    with open(label_file, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        next(reader)  # Skip the header\n",
        "        for row in reader:\n",
        "            image_path = os.path.join(image_folder, row[0])\n",
        "            if os.path.exists(image_path):  # Check if the image file exists\n",
        "                image = Image.open(image_path).convert('L')  # Convert image to grayscale\n",
        "                image = image.resize((50, 50))  # Resize image\n",
        "                images.append(np.array(image).flatten())\n",
        "                labels.append(row[1])\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Load data\n",
        "images, labels = load_data('/content/drive/MyDrive/Dataset/Faces/Faces', '/content/drive/MyDrive/Dataset/Dataset.csv')\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply PCA to reduce dimensionality\n",
        "pca = PCA(n_components=100)\n",
        "X_train = pca.fit_transform(X_train)\n",
        "X_test = pca.transform(X_test)\n",
        "\n",
        "# Train a MLP classifier\n",
        "clf = MLPClassifier(hidden_layer_sizes=(200, 300), max_iter=500, alpha=0.0001,\n",
        "                     solver='sgd', verbose=10,  random_state=21,tol=0.00000001)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"The accuracy of the model is {accuracy * 100}%.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
